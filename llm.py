"""
LLMå®¢æˆ·ç«¯æ¨¡å—
=============

æä¾›ä¸å¤§è¯­è¨€æ¨¡å‹äº¤äº’çš„å®¢æˆ·ç«¯ï¼Œæ”¯æŒå¤šç§APIæä¾›å•†ã€‚
ç±»æ¯”pytestçš„æµ‹è¯•è¿è¡Œå™¨ï¼Œè´Ÿè´£æ‰§è¡Œæ ¸å¿ƒé€»è¾‘ã€‚

è®¾è®¡ç†å¿µï¼š
1. ç»Ÿä¸€æ¥å£ï¼Œæ”¯æŒå¤šç§LLMæä¾›å•†
2. å¼‚æ­¥æ”¯æŒï¼Œæé«˜æ€§èƒ½
3. é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
4. Mockæ¨¡å¼ï¼Œä¾¿äºæµ‹è¯•å’Œæ¼”ç¤º
"""

import asyncio
from typing import Dict, Any
from abc import ABC, abstractmethod


class BaseLLMClient(ABC):
    """
    LLMå®¢æˆ·ç«¯åŸºç±»
    
    ç±»æ¯”pytestçš„æµ‹è¯•è¿è¡Œå™¨åŸºç±»ï¼Œå®šä¹‰ç»Ÿä¸€æ¥å£
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.model = config.get('model', 'unknown')
        self.api_key = config.get('api_key', '')
        self.base_url = config.get('base_url', '')
        self.max_tokens = config.get('max_tokens', 1000)
        self.temperature = config.get('temperature', 0.7)
    
    @abstractmethod
    async def generate(self, system_prompt: str, user_prompt: str) -> str:
        """
        ç”Ÿæˆæ–‡æœ¬å“åº”
        
        Args:
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            user_prompt: ç”¨æˆ·æç¤ºè¯
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        pass


# Mock å®¢æˆ·ç«¯å·²ç§»é™¤ï¼Œç°åœ¨åªä½¿ç”¨çœŸå®çš„ LLM API


class OpenAIClient(BaseLLMClient):
    """
    OpenAI APIå®¢æˆ·ç«¯
    
    çœŸå®çš„OpenAI APIè°ƒç”¨å®ç° (éœ€è¦å®‰è£…openaiåº“)
    """
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        try:
            import openai
            self.client = openai.AsyncOpenAI(
                api_key=self.api_key,
                base_url=self.base_url
            )
        except ImportError:
            raise ImportError("éœ€è¦å®‰è£…openaiåº“: pip install openai")
    
    async def generate(self, system_prompt: str, user_prompt: str) -> str:
        """ä½¿ç”¨OpenAI APIç”Ÿæˆå“åº”"""
        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=self.max_tokens,
                temperature=self.temperature
            )
            return response.choices[0].message.content
        except Exception as e:
            raise RuntimeError(f"OpenAI APIè°ƒç”¨å¤±è´¥: {str(e)}")


class AnthropicClient(BaseLLMClient):
    """
    Anthropic Claude APIå®¢æˆ·ç«¯
    
    çœŸå®çš„Claude APIè°ƒç”¨å®ç° (éœ€è¦å®‰è£…anthropicåº“)
    """
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        try:
            import anthropic
            self.client = anthropic.AsyncAnthropic(
                api_key=self.api_key,
                base_url=self.base_url
            )
        except ImportError:
            raise ImportError("éœ€è¦å®‰è£…anthropicåº“: pip install anthropic")
    
    async def generate(self, system_prompt: str, user_prompt: str) -> str:
        """ä½¿ç”¨Anthropic APIç”Ÿæˆå“åº”"""
        try:
            response = await self.client.messages.create(
                model=self.model,
                system=system_prompt,
                messages=[{"role": "user", "content": user_prompt}],
                max_tokens=self.max_tokens,
                temperature=self.temperature
            )
            return response.content[0].text
        except Exception as e:
            raise RuntimeError(f"Anthropic APIè°ƒç”¨å¤±è´¥: {str(e)}")


class LLMClient:
    """
    LLMå®¢æˆ·ç«¯å·¥å‚ç±»
    
    æ ¹æ®é…ç½®è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„LLMå®¢æˆ·ç«¯
    ç±»æ¯”pytestçš„æ’ä»¶åŠ è½½æœºåˆ¶
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.client = self._create_client()
    
    def _create_client(self) -> BaseLLMClient:
        """
        æ ¹æ®é…ç½®åˆ›å»ºLLMå®¢æˆ·ç«¯

        Returns:
            LLMå®¢æˆ·ç«¯å®ä¾‹
        """
        model = self.config.get('model', '').lower()
        api_type = self.config.get('api_type', '').lower()
        base_url = self.config.get('base_url', '').lower()

        # æ ¹æ®æ¨¡å‹åç§°ã€APIç±»å‹æˆ–base_urlé€‰æ‹©å®¢æˆ·ç«¯
        if 'claude' in model or 'anthropic' in api_type:
            print(f"ğŸ”§ ä½¿ç”¨ Anthropic å®¢æˆ·ç«¯ï¼Œæ¨¡å‹: {self.config.get('model')}")
            return AnthropicClient(self.config)
        else:
            # é»˜è®¤ä½¿ç”¨ OpenAI å…¼å®¹å®¢æˆ·ç«¯ (æ”¯æŒ OpenAI, Qwen, GLM, ç¡…åŸºæµåŠ¨ç­‰)
            print(f"ğŸ”§ ä½¿ç”¨ OpenAI å…¼å®¹å®¢æˆ·ç«¯ï¼Œæ¨¡å‹: {self.config.get('model')}")
            return OpenAIClient(self.config)
    
    async def generate(self, system_prompt: str, user_prompt: str) -> str:
        """
        ç”Ÿæˆæ–‡æœ¬å“åº”
        
        Args:
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            user_prompt: ç”¨æˆ·æç¤ºè¯
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        return await self.client.generate(system_prompt, user_prompt)
    
    def get_model_info(self) -> Dict[str, Any]:
        """
        è·å–æ¨¡å‹ä¿¡æ¯
        
        Returns:
            æ¨¡å‹ä¿¡æ¯å­—å…¸
        """
        return {
            'model': self.client.model,
            'type': self.client.__class__.__name__,
            'max_tokens': self.client.max_tokens,
            'temperature': self.client.temperature
        }


# ä¾¿æ·å‡½æ•°
async def create_llm_client(config: Dict[str, Any]) -> LLMClient:
    """
    åˆ›å»ºLLMå®¢æˆ·ç«¯çš„ä¾¿æ·å‡½æ•°
    
    Args:
        config: é…ç½®å­—å…¸
        
    Returns:
        LLMå®¢æˆ·ç«¯å®ä¾‹
    """
    return LLMClient(config)


# æµ‹è¯•å‡½æ•°
if __name__ == "__main__":
    async def test_llm_clients():
        """æµ‹è¯•LLMå®¢æˆ·ç«¯é…ç½®"""
        from config import load_config

        print("ğŸ§ª æµ‹è¯•LLMå®¢æˆ·ç«¯é…ç½®:")
        print("-" * 30)

        config = load_config()
        client = LLMClient(config)

        print(f"ğŸ“Š æ¨¡å‹ä¿¡æ¯: {client.get_model_info()}")
        print("\nâœ… é…ç½®æµ‹è¯•å®Œæˆ")
        print("ğŸ’¡ è¦æµ‹è¯•å®é™…APIè°ƒç”¨ï¼Œè¯·ç¡®ä¿é…ç½®äº†æœ‰æ•ˆçš„APIå¯†é’¥")

    asyncio.run(test_llm_clients())